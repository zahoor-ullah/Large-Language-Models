{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f22f2ac9-517c-47c4-9735-1bbdcea7e23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5c95900-b153-472f-ae27-808f4c2282ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created index with 10 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zahor/anaconda3/envs/ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# generate embedding\n",
    "def generate_embedding(docs, model, tokenizer):\n",
    "    # Tokenize each text and convert to PyTorch tensors\n",
    "    inputs = tokenizer(docs, padding=True,truncation=True, return_tensors=\"pt\",max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Embedding defined aas mean pooling oof all tokens\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    embeddings = outputs.last_hidden_state\n",
    "\n",
    "    expanded_mask = attention_mask.unsqueeze(-1).expand(embeddings.shape).float()\n",
    "    sum_embeddings = torch.sum(embeddings * expanded_mask,axis=1)\n",
    "    sum_mask = torch.clamp(expanded_mask.sum(axis=1), min=1e-9)\n",
    "    mean_embeddings = sum_embeddings / sum_mask\n",
    "\n",
    "    # Convert to numpy array\n",
    "    return mean_embeddings.cpu().numpy()\n",
    "\n",
    "# Sample document collection\n",
    "documents = [\n",
    "    \"Transformers are a type of deep learning model introduced in the paper 'Attention \"\n",
    "        \"Is All You Need'.\",\n",
    "    \"BERT (Bidirectional Encoder Representations from Transformers) is a \"\n",
    "        \"transformer-based model designed to understand the context of a word based on \"\n",
    "        \"its surroundings.\",\n",
    "    \"GPT (Generative Pre-trained Transformer) is a transformer-based model designed for \"\n",
    "        \"natural language generation tasks.\",\n",
    "    \"T5 (Text-to-Text Transfer Transformer) treats every NLP problem as a text-to-text \"\n",
    "        \"problem, where both the input and output are text strings.\",\n",
    "    \"RoBERTa is an optimized version of BERT with improved training methodology and more \"\n",
    "        \"training data.\",\n",
    "    \"DistilBERT is a smaller, faster version of BERT that retains 97% of its language \"\n",
    "        \"understanding capabilities.\",\n",
    "    \"ALBERT reduces the parameters of BERT by sharing parameters across layers and using \"\n",
    "        \"embedding factorization.\",\n",
    "    \"XLNet is a generalized autoregressive pretraining method that overcomes the \"\n",
    "        \"limitations of BERT by using permutation language modeling.\",\n",
    "    \"ELECTRA uses a generator-discriminator architecture for more efficient pretraining.\",\n",
    "    \"DeBERTa enhances BERT with disentangled attention and an enhanced mask decoder.\"\n",
    "]\n",
    "# Generate embeddings for all documents, then create FAISS index for efficient similarity search\n",
    "document_embeddings = generate_embedding(documents, model, tokenizer)\n",
    "dimension = document_embeddings.shape[1]   # Dimension of the embeddings\n",
    "index = faiss.IndexFlatL2(dimension)       # Using L2 (Euclidean) distance\n",
    "index.add(document_embeddings)             # Add embeddings to the index\n",
    "print(f\"Created index with {index.ntotal} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ae956b3-1fe5-4340-995f-f3c48892a940",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embeddings = generate_embedding(documents, model, tokenizer)\n",
    "normalized = document_embeddings / np.linalg.norm(document_embeddings, axis=1, keepdims=True)\n",
    "index.add(normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550473ed-66ce-4115-9fa8-2aa89d3ae07c",
   "metadata": {},
   "source": [
    "### **Implementing the Retrieval System**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3101083-6d3e-4edc-9ad5-49e32eaed8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is BERT?\n",
      "\n",
      "Document 1 (Distance: 23.7060):\n",
      "BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model designed to understand the context of a word based on its surroundings.\n",
      "\n",
      "Document 2 (Distance: 28.0794):\n",
      "RoBERTa is an optimized version of BERT with improved training methodology and more training data.\n",
      "\n",
      "Document 3 (Distance: 29.5908):\n",
      "DistilBERT is a smaller, faster version of BERT that retains 97% of its language understanding capabilities.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "...\n",
    "\n",
    "def retrieve_documents(query, index, documents, k=3):\n",
    "    # Generate embedding for the query\n",
    "    query_embedding = generate_embedding(query, model, tokenizer)   # 1xD matrix\n",
    "    # Search the index for similar documents\n",
    "    distances, indices = index.search(query_embedding, k)  # 1xk matrices\n",
    "    # Return the retrieved documents and their distances\n",
    "    retrieved_docs = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        if 0 <= idx < len(documents):  # make sure index is valid\n",
    "            retrieved_docs.append((documents[idx], float(distances[0][i])))\n",
    "\n",
    "    return retrieved_docs\n",
    "\n",
    "# Example query\n",
    "query = \"What is BERT?\"\n",
    "retrieved_docs = retrieve_documents(query, index, documents)\n",
    "\n",
    "# Print the retrieved documents\n",
    "print(f\"Query: {query}\\n\")\n",
    "for i, (doc, distance) in enumerate(retrieved_docs):\n",
    "    print(f\"Document {i+1} (Distance: {distance:.4f}):\")\n",
    "    print(doc)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559f51b6-c402-4a17-ad10-991b2d518acf",
   "metadata": {},
   "source": [
    "### **Implementing the Generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03af0fdd-2f3f-4fbc-9839-f0e7dede8a2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Generate a response for the example query\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mretrieved_docs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Response:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "Cell \u001b[0;32mIn[37], line 8\u001b[0m, in \u001b[0;36mgenerate_response\u001b[0;34m(query, retrieved_docs, max_length)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_response\u001b[39m(query, retrieved_docs, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Combine the query and retrieved documents into a single prompt\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m retrieved_docs])\n\u001b[1;32m      9\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m context: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Generate a response\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[37], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_response\u001b[39m(query, retrieved_docs, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Combine the query and retrieved documents into a single prompt\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m retrieved_docs])\n\u001b[1;32m      9\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m context: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Generate a response\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "gen_model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "\n",
    "def generate_response(query, retrieved_docs, max_length=150):\n",
    "    # Combine the query and retrieved documents into a single prompt\n",
    "    context = \"\\n\".join([doc for doc, _ in retrieved_docs])\n",
    "    prompt = f\"question: {query} context: {context}\"\n",
    "\n",
    "    # Generate a response\n",
    "    inputs = gen_tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = gen_model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=max_length,\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "    response = gen_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Generate a response for the example query\n",
    "response = generate_response(query, [doc for doc, score in retrieved_docs])\n",
    "print(\"Generated Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4ed6a0-b6e2-4ca4-a8e3-c8c4608eff1c",
   "metadata": {},
   "source": [
    "## ****Building the Complete RAG System****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac440cd4-1602-4866-8fd8-bfa02f2a4754",
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "def rag_pipeline(query, documents, retriever_k=3, max_length=150):\n",
    "    retrieved_docs = retrieve_documents(query, index, documents, k=retriever_k)\n",
    "    response = generate_response(query, retrieved_docs, max_length=max_length)\n",
    "    return response, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74333e28-b885-4384-8b6c-fa566c814040",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, tuple found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Run the RAG pipeline for each query\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m queries:\n\u001b[0;32m---> 12\u001b[0m     response, retrieved_docs \u001b[38;5;241m=\u001b[39m \u001b[43mrag_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n",
      "Cell \u001b[0;32mIn[35], line 4\u001b[0m, in \u001b[0;36mrag_pipeline\u001b[0;34m(query, documents, retriever_k, max_length)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrag_pipeline\u001b[39m(query, documents, retriever_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m):\n\u001b[1;32m      3\u001b[0m     retrieved_docs \u001b[38;5;241m=\u001b[39m retrieve_documents(query, index, documents, k\u001b[38;5;241m=\u001b[39mretriever_k)\n\u001b[0;32m----> 4\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretrieved_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response, retrieved_docs\n",
      "Cell \u001b[0;32mIn[34], line 8\u001b[0m, in \u001b[0;36mgenerate_response\u001b[0;34m(query, retrieved_docs, max_length)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_response\u001b[39m(query, retrieved_docs, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Combine the query and retrieved documents into a single prompt\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretrieved_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m context: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Generate a response\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, tuple found"
     ]
    }
   ],
   "source": [
    "...\n",
    "\n",
    "# Example queries\n",
    "queries = [\n",
    "    \"What is BERT?\",\n",
    "    \"How does GPT work?\",\n",
    "    \"What is the difference between BERT and GPT?\",\n",
    "    \"What is a smaller version of BERT?\"\n",
    "]\n",
    "# Run the RAG pipeline for each query\n",
    "for query in queries:\n",
    "    response, retrieved_docs = rag_pipeline(query, documents)\n",
    "    print(f\"Query: {query}\")\n",
    "    print()\n",
    "    print(\"Retrieved Documents:\")\n",
    "    for i, (doc, distance) in enumerate(retrieved_docs):\n",
    "        print(f\"Document {i+1} (Distance: {distance:.4f}):\")\n",
    "        print(doc)\n",
    "    print()\n",
    "    print(\"Generated Response:\")\n",
    "    print(response)\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25087e24-53da-4cf7-abff-a34ea58795ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fc803c-69e9-41b2-9343-1868de83266b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c8a4f9-2586-468f-a98a-da89dfe56993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3487f8c0-e42d-4cff-b5bf-53f6127cad5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96000278-557e-411d-87a9-996159db63bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83776f83-1309-4b4b-ba96-5e430d7c2568",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
