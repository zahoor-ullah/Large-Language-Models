{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fa7c871-d820-4add-8643-fc61d72284d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "co = cohere.Client(\"j3dnErL21HLrrwoaSskJsQpmAn2SxRc1JzJWg7yL\") # Get your API key here: https://dashboard.cohere.com/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1d03430-310c-4318-8f15-9208dd125934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define documents\n",
    "documents = [\n",
    "    {\n",
    "        \"title\": \"Tall penguins\",\n",
    "        \"text\": \"Emperor penguins are the tallest.\"},\n",
    "    {\n",
    "        \"title\": \"Penguin habitats\",\n",
    "        \"text\": \"Emperor penguins only live in Antarctica.\"},\n",
    "    {\n",
    "        \"title\": \"What are animals?\",\n",
    "        \"text\": \"Animals are different from plants.\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a76a655-cb84-4980-97cd-5ba35740d0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emperor penguins are the tallest living penguins.\n",
      "\n",
      "CITATIONS:\n",
      "start=0 end=16 text='Emperor penguins' document_ids=['doc_0'] type='TEXT_CONTENT'\n",
      "\n",
      "DOCUMENTS:\n",
      "{'id': 'doc_0', 'text': 'Emperor penguins are the tallest.', 'title': 'Tall penguins'}\n"
     ]
    }
   ],
   "source": [
    "# Generate response with citations\n",
    "\n",
    "# Get the use message \n",
    "message = \"What are the tallest living penguins?\"\n",
    "\n",
    "# Generat the response\n",
    "response = co.chat_stream(\n",
    "    message = message,\n",
    "    model = \"command-a-03-2025\",\n",
    "    documents = documents\n",
    ")\n",
    "# Display the response\n",
    "citations = []\n",
    "cited_documents = []\n",
    "for event in response:\n",
    "    if event.event_type == \"text-generation\":\n",
    "        print(event.text, end=\"\")\n",
    "    elif event.event_type == \"citation-generation\":\n",
    "        citations.extend(event.citations)\n",
    "    elif event.event_type == \"stream-end\":\n",
    "      cited_documents = event.response.documents\n",
    "\n",
    "# Display the citations and source documents\n",
    "if citations:\n",
    "  print(\"\\n\\nCITATIONS:\")\n",
    "  for citation in citations:\n",
    "    print(citation)\n",
    "\n",
    "  print(\"\\nDOCUMENTS:\")\n",
    "  for document in cited_documents:\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5e3548-4640-4807-a8f4-4dec9de6ae79",
   "metadata": {},
   "source": [
    "## **RAG-Powered Chatbot with Chat, Embed, and Rerank**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbaaa7b-ef9f-4e6d-9e87-c62e86df8778",
   "metadata": {},
   "source": [
    "#### Summary of steps involved\n",
    "* Step 0: Ingest the documents – get documents, chunk, embed, and index.\n",
    "* Step 1: Get the user message\n",
    "* Step 2: Call the Chat endpoint in query-generation mode\n",
    "* If at least one query is generated\n",
    "    * Step 3: Retrieve and rerank relevant documents\n",
    "    * Step 4: Call the Chat endpoint in document mode to generate a grounded response with citations\n",
    "    * If no query is generated\n",
    "    * Step 4: Call the Chat endpoint in normal mode to generate a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d44635e-2e0b-4198-b333-e7275fd8e0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "import uuid\n",
    "import hnswlib\n",
    "from typing import List, Dict\n",
    "from unstructured.partition.html import partition_html\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "\n",
    "co = cohere.Client(\"j3dnErL21HLrrwoaSskJsQpmAn2SxRc1JzJWg7yL\") # Get your API key here: https://dashboard.cohere.com/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce94191f-e3fc-4d4c-b33a-e5cc88fd4e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector store for ingestion and retrieval\n",
    "raw_documents = [\n",
    "    {\n",
    "        \"title\": \"Crafting Effective Prompts\",\n",
    "        \"url\": \"https://docs.cohere.com/docs/crafting-effective-prompts\"},\n",
    "    {\n",
    "        \"title\": \"Advanced Prompt Engineering Techniques\",\n",
    "        \"url\": \"https://docs.cohere.com/docs/advanced-prompt-engineering-techniques\"},\n",
    "    {\n",
    "        \"title\": \"Prompt Truncation\",\n",
    "        \"url\": \"https://docs.cohere.com/docs/prompt-truncation\"},\n",
    "    {\n",
    "        \"title\": \"Preambles\",\n",
    "        \"url\": \"https://docs.cohere.com/docs/preambles\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5dfaf4b-cdf4-4c5f-9a7e-79d54d12251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorstore:\n",
    "    \"\"\"\n",
    "    A class representing a collection of documents indexed into a vectorstore.\n",
    "\n",
    "    Parameters:\n",
    "    raw_documents (list): A list of dictionaries representing the sources of the raw documents. Each dictionary should have 'title' and 'url' keys.\n",
    "\n",
    "    Attributes:\n",
    "    raw_documents (list): A list of dictionaries representing the raw documents.\n",
    "    docs (list): A list of dictionaries representing the chunked documents, with 'title', 'text', and 'url' keys.\n",
    "    docs_embs (list): A list of the associated embeddings for the document chunks.\n",
    "    docs_len (int): The number of document chunks in the collection.\n",
    "    idx (hnswlib.Index): The index used for document retrieval.\n",
    "\n",
    "    Methods:\n",
    "    load_and_chunk(): Loads the data from the sources and partitions the HTML content into chunks.\n",
    "    embed(): Embeds the document chunks using the Cohere API.\n",
    "    index(): Indexes the document chunks for efficient retrieval.\n",
    "    retrieve(): Retrieves document chunks based on the given query.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, raw_documents: List[Dict[str, str]]):\n",
    "        self.raw_documents = raw_documents\n",
    "        self.docs = []\n",
    "        self.docs_embs = []\n",
    "        self.retrieve_top_k = 10\n",
    "        self.rerank_top_k = 3\n",
    "        self.load_and_chunk()\n",
    "        self.embed()\n",
    "        self.index()\n",
    "\n",
    "\n",
    "    def load_and_chunk(self) -> None:\n",
    "        \"\"\"\n",
    "        Loads the text from the sources and chunks the HTML content.\n",
    "        \"\"\"\n",
    "        print(\"Loading documents...\")\n",
    "\n",
    "        for raw_document in self.raw_documents:\n",
    "            elements = partition_html(url=raw_document[\"url\"])\n",
    "            chunks = chunk_by_title(elements)\n",
    "            for chunk in chunks:\n",
    "                self.docs.append(\n",
    "                    {\n",
    "                        \"title\": raw_document[\"title\"],\n",
    "                        \"text\": str(chunk),\n",
    "                        \"url\": raw_document[\"url\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    def embed(self) -> None:\n",
    "        \"\"\"\n",
    "        Embeds the document chunks using the Cohere API.\n",
    "        \"\"\"\n",
    "        print(\"Embedding document chunks...\")\n",
    "\n",
    "        batch_size = 90\n",
    "        self.docs_len = len(self.docs)\n",
    "        for i in range(0, self.docs_len, batch_size):\n",
    "            batch = self.docs[i : min(i + batch_size, self.docs_len)]\n",
    "            texts = [item[\"text\"] for item in batch]\n",
    "            docs_embs_batch = co.embed(\n",
    "                texts=texts, model=\"embed-v4.0\", input_type=\"search_document\"\n",
    "            ).embeddings\n",
    "            self.docs_embs.extend(docs_embs_batch)\n",
    "\n",
    "    def index(self) -> None:\n",
    "        \"\"\"\n",
    "        Indexes the document chunks for efficient retrieval.\n",
    "        \"\"\"\n",
    "        print(\"Indexing document chunks...\")\n",
    "    \n",
    "        # Determine embedding dimensionality from first vector\n",
    "        dim = len(self.docs_embs[0])\n",
    "        \n",
    "        self.idx = hnswlib.Index(space=\"ip\", dim=dim)\n",
    "        self.idx.init_index(max_elements=self.docs_len, ef_construction=512, M=64)\n",
    "        self.idx.add_items(self.docs_embs, list(range(len(self.docs_embs))))\n",
    "    \n",
    "        print(f\"Indexing complete with {self.idx.get_current_count()} document chunks (dim={dim}).\")\n",
    "\n",
    "    def retrieve(self, query: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Retrieves document chunks based on the given query.\n",
    "\n",
    "        Parameters:\n",
    "        query (str): The query to retrieve document chunks for.\n",
    "\n",
    "        Returns:\n",
    "        List[Dict[str, str]]: A list of dictionaries representing the retrieved document chunks, with 'title', 'text', and 'url' keys.\n",
    "        \"\"\"\n",
    "\n",
    "        # Dense retrieval\n",
    "        query_emb = co.embed(\n",
    "            texts=[query], model=\"embed-v4.0\", input_type=\"search_query\"\n",
    "        ).embeddings\n",
    "        \n",
    "        doc_ids = self.idx.knn_query(query_emb, k=self.retrieve_top_k)[0][0]\n",
    "\n",
    "        # Reranking\n",
    "        rank_fields = [\"title\", \"text\"] # We'll use the title and text fields for reranking\n",
    "\n",
    "        docs_to_rerank = [self.docs[doc_id] for doc_id in doc_ids]\n",
    "        rerank_results = co.rerank(\n",
    "            query=query,\n",
    "            documents=docs_to_rerank,\n",
    "            top_n=self.rerank_top_k,\n",
    "            model=\"rerank-english-v3.0\",\n",
    "            rank_fields=rank_fields\n",
    "        )\n",
    "\n",
    "        doc_ids_reranked = [doc_ids[result.index] for result in rerank_results.results]\n",
    "\n",
    "        docs_retrieved = []\n",
    "        for doc_id in doc_ids_reranked:\n",
    "            docs_retrieved.append(\n",
    "                {\n",
    "                    \"title\": self.docs[doc_id][\"title\"],\n",
    "                    \"text\": self.docs[doc_id][\"text\"],\n",
    "                    \"url\": self.docs[doc_id][\"url\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return docs_retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6021086a-c451-4dfa-bcae-b9084c5517da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents...\n",
      "Embedding document chunks...\n",
      "Indexing document chunks...\n",
      "Indexing complete with 131 document chunks (dim=1536).\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the Vectorstore class with the given sources\n",
    "vectorstore = Vectorstore(raw_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd4dd82-f99b-4ae7-ad3e-9e47dbfeab5f",
   "metadata": {},
   "source": [
    "## Dense retrieval\n",
    "    * First, we embed the query using the same embed-v4.0 model we used to embed the document chunks, but this time we set input_type=\"search_query\".\n",
    "\n",
    " * Search is performed by the knn_query() method from the hnswlib library. Given a query, it returns the document chunks most similar to the query. We can define the number of document chunks to return using the attribute self.retrieve_top_k=10.\n",
    "\n",
    "## Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db5aec2f-d01a-4ed5-9eed-f1911743bea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Advanced Prompt Engineering Techniques',\n",
       "  'text': 'Few-shot Prompting\\n\\nUnlike the zero-shot examples above, few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. We can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.',\n",
       "  'url': 'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques'},\n",
       " {'title': 'Crafting Effective Prompts',\n",
       "  'text': 'Incorporating Example Outputs\\n\\nLLMs respond well when they have specific examples to work from. For example, instead of asking for the salient points of the text and using bullet points “where appropriate”, give an example of what the output should look like.',\n",
       "  'url': 'https://docs.cohere.com/docs/crafting-effective-prompts'},\n",
       " {'title': 'Advanced Prompt Engineering Techniques',\n",
       "  'text': 'In addition to giving correct examples, including negative examples with a clear indication of why they are wrong can help the LLM learn to distinguish between correct and incorrect responses. Ordering the examples can also be important; if there are patterns that could be picked up on that are not relevant to the correctness of the question, the model may incorrectly pick up on those instead of the semantics of the question itself.',\n",
       "  'url': 'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Retrieval\n",
    "vectorstore.retrieve(\"Prompting by giving examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995163b8-5814-49a4-8a1b-633d1ebef88c",
   "metadata": {},
   "source": [
    "### Run chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d39f855c-24c5-4f68-ade9-7b697ec3ee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chatbot(message, chat_history=None):\n",
    "    if chat_history is None:\n",
    "        chat_history = []\n",
    "    \n",
    "    # Generate search queries, if any        \n",
    "    response = co.chat(message=message,\n",
    "                        model=\"command-a-03-2025\",\n",
    "                        search_queries_only=True,\n",
    "                        chat_history=chat_history)\n",
    "    \n",
    "    search_queries = []\n",
    "    for query in response.search_queries:\n",
    "        search_queries.append(query.text)\n",
    "\n",
    "    # If there are search queries, retrieve the documents\n",
    "    if search_queries:\n",
    "        print(\"Retrieving information...\", end=\"\")\n",
    "\n",
    "        # Retrieve document chunks for each query\n",
    "        documents = []\n",
    "        for query in search_queries:\n",
    "            documents.extend(vectorstore.retrieve(query))\n",
    "\n",
    "        # Use document chunks to respond\n",
    "        response = co.chat_stream(\n",
    "            message=message,\n",
    "            model=\"command-a-03-2025\",\n",
    "            documents=documents,\n",
    "            chat_history=chat_history,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        response = co.chat_stream(\n",
    "            message=message,\n",
    "            model=\"command-a-03-2025\",\n",
    "            chat_history=chat_history,\n",
    "        )\n",
    "        \n",
    "    # Print the chatbot response, citations, and documents\n",
    "    chatbot_response = \"\"\n",
    "    print(\"\\nChatbot:\")\n",
    "\n",
    "    for event in response:\n",
    "        if event.event_type == \"text-generation\":\n",
    "            print(event.text, end=\"\")\n",
    "            chatbot_response += event.text\n",
    "        if event.event_type == \"stream-end\":\n",
    "            if event.response.citations:\n",
    "                print(\"\\n\\nCITATIONS:\")\n",
    "                for citation in event.response.citations:\n",
    "                    print(citation)\n",
    "            if event.response.documents:\n",
    "                print(\"\\nCITED DOCUMENTS:\")\n",
    "                for document in event.response.documents:\n",
    "                    print(document)\n",
    "            # Update the chat history for the next turn\n",
    "            chat_history = event.response.chat_history\n",
    "\n",
    "    return chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28563381-9a1e-4f11-b4ae-b8d93bf6a65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chatbot:\n",
      "Hello! I'm here to help. Please go ahead and ask your question, and I'll do my best to provide a helpful and informative answer."
     ]
    }
   ],
   "source": [
    "# Turn # 1\n",
    "chat_history = run_chatbot(\"Hello, I have a question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c53f4ef-c8da-46c1-a185-db6dc7476f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chatbot:\n",
      "Great question! Zero-shot and few-shot prompting are techniques used in natural language processing (NLP) to guide language models in generating or classifying text without explicit fine-tuning. Here’s the difference between the two:\n",
      "\n",
      "### **Zero-Shot Prompting**\n",
      "- **Definition**: Zero-shot prompting involves providing the model with a task or question without any examples. The model relies solely on its pre-trained knowledge to generate a response.\n",
      "- **Example**: Asking a model, *\"What is the capital of France?\"* without providing any context or examples of similar questions.\n",
      "- **Use Case**: Useful when the model is expected to generalize its knowledge to new tasks or questions it hasn't explicitly seen before.\n",
      "- **Advantage**: Requires no additional training data or examples, making it efficient for quick queries.\n",
      "- **Limitation**: May not perform well on highly specific or complex tasks where examples could improve understanding.\n",
      "\n",
      "### **Few-Shot Prompting**\n",
      "- **Definition**: Few-shot prompting involves providing the model with a few examples (typically 1–10) of the task or question format before asking it to perform the task. These examples act as a guide for the model.\n",
      "- **Example**: Providing the model with:  \n",
      "  *\"Q: What is the capital of France? A: Paris\"*  \n",
      "  *\"Q: What is the capital of Japan? A: Tokyo\"*  \n",
      "  Then asking: *\"Q: What is the capital of Germany?\"*\n",
      "- **Use Case**: Useful when the task is specific or when the model needs additional context to perform accurately.\n",
      "- **Advantage**: Improves performance on tasks where examples help the model understand the expected format or reasoning.\n",
      "- **Limitation**: Requires crafting relevant examples, which can be time-consuming or require domain knowledge.\n",
      "\n",
      "### **Key Differences**\n",
      "| **Aspect**          | **Zero-Shot**                          | **Few-Shot**                          |\n",
      "|----------------------|----------------------------------------|---------------------------------------|\n",
      "| **Examples Provided**| None                                  | A few (1–10)                         |\n",
      "| **Reliance on Data** | Relies entirely on pre-trained knowledge | Uses examples to guide the model     |\n",
      "| **Complexity**       | Simpler to implement                  | Requires careful selection of examples|\n",
      "| **Performance**      | May struggle with complex tasks       | Generally better for specific tasks  |\n",
      "\n",
      "Both techniques leverage the model's ability to generalize, but few-shot prompting provides additional context to improve accuracy and relevance. Let me know if you'd like further clarification!"
     ]
    }
   ],
   "source": [
    "# Turn # 2\n",
    "chat_history = run_chatbot(\"What's the difference between zero-shot and few-shot prompting\", chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f405d93-3773-4907-b305-709d5634a46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chatbot:\n",
      "Few-shot prompting helps by providing the language model with a small set of examples that illustrate the task or question format. These examples act as a **contextual guide**, allowing the model to better understand what is expected of it. Here’s how few-shot prompting improves performance:\n",
      "\n",
      "### **1. Clarifies Task Expectations**\n",
      "   - The examples in few-shot prompting explicitly show the model the input-output pairs or the reasoning process required for the task.\n",
      "   - For instance, if you’re asking the model to classify sentiment, providing examples like *\"Text: 'I love this movie!' Sentiment: Positive\"* helps the model understand the format and criteria for classification.\n",
      "\n",
      "### **2. Improves Generalization**\n",
      "   - While the model is pre-trained on a vast corpus of text, few-shot examples help it **adapt** to the specific nuances of the task at hand.\n",
      "   - This is particularly useful for tasks that are not well-represented in the pre-training data or require domain-specific knowledge.\n",
      "\n",
      "### **3. Reduces Ambiguity**\n",
      "   - Without examples, the model might generate responses based on its general understanding, which could be ambiguous or incorrect for specific tasks.\n",
      "   - Few-shot examples **reduce ambiguity** by explicitly demonstrating the desired output style, tone, or structure.\n",
      "\n",
      "### **4. Enhances Consistency**\n",
      "   - By providing examples, you ensure the model’s responses are more consistent with the intended task.\n",
      "   - For example, in a summarization task, examples can show the model the desired length, level of detail, and tone of the summary.\n",
      "\n",
      "### **5. Enables Complex Reasoning**\n",
      "   - For tasks requiring multi-step reasoning or specific logic, few-shot examples can illustrate the thought process.\n",
      "   - For instance, in a math word problem, showing the model how to break down the problem and solve it step-by-step improves its ability to handle similar questions.\n",
      "\n",
      "### **6. Adapts to New Tasks**\n",
      "   - Few-shot prompting is particularly useful for **novel tasks** that the model hasn’t encountered during pre-training.\n",
      "   - By providing a few examples, you can quickly \"teach\" the model how to perform the new task without needing to fine-tune it on a large dataset.\n",
      "\n",
      "### **Example Scenario**\n",
      "**Task**: Translate English to French in a formal tone.  \n",
      "**Few-Shot Examples**:  \n",
      "- *\"English: 'Thank you for your assistance.' French: 'Merci pour votre assistance.'\"*  \n",
      "- *\"English: 'Could you please provide more details?' French: 'Pourriez-vous fournir plus de détails, s’il vous plaît ?'\"*  \n",
      "\n",
      "**Prompt**: *\"English: 'We appreciate your prompt response.' French:\"*  \n",
      "\n",
      "**Outcome**: The model is more likely to produce a formal and accurate translation like *\"Nous apprécions votre réponse rapide.\"* because the examples guided it on tone and structure.\n",
      "\n",
      "In summary, few-shot prompting acts as a **bridge** between the model’s general knowledge and the specific requirements of the task, leading to more accurate, relevant, and consistent responses."
     ]
    }
   ],
   "source": [
    "# Turn # 3\n",
    "chat_history = run_chatbot(\"How would the latter help?\", chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c902ecb6-6d76-4e52-8862-21157e281caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chatbot:\n",
      "Great question! 5G networks represent the fifth generation of wireless communication technology, designed to significantly enhance speed, capacity, and responsiveness compared to previous generations (like 4G LTE). Here’s a breakdown of what you need to know about 5G:\n",
      "\n",
      "---\n",
      "\n",
      "### **Key Features of 5G Networks**\n",
      "1. **Higher Speeds**:\n",
      "   - 5G offers **peak speeds** of up to 20 Gbps (gigabits per second), though real-world speeds are typically lower but still much faster than 4G.\n",
      "   - This enables seamless streaming of 4K/8K video, faster downloads, and improved performance for data-intensive applications.\n",
      "\n",
      "2. **Lower Latency**:\n",
      "   - 5G reduces **latency** (delay in data transmission) to as low as **1 millisecond** (compared to 20–30 ms for 4G).\n",
      "   - This is crucial for real-time applications like autonomous vehicles, remote surgery, and online gaming.\n",
      "\n",
      "3. **Increased Capacity**:\n",
      "   - 5G supports a **higher density of connected devices** per square kilometer, making it ideal for the Internet of Things (IoT) and smart cities.\n",
      "   - It can handle up to **1 million devices per square kilometer**, compared to 4G's limit of around 100,000.\n",
      "\n",
      "4. **Network Slicing**:\n",
      "   - 5G allows **network slicing**, where a single physical network is divided into multiple virtual networks tailored to specific needs (e.g., one slice for IoT devices, another for high-speed data).\n",
      "\n",
      "5. **Improved Reliability**:\n",
      "   - 5G is designed to be more reliable, with **99.999% uptime**, making it suitable for mission-critical applications like industrial automation and emergency services.\n",
      "\n",
      "---\n",
      "\n",
      "### **How 5G Works**\n",
      "5G operates on three main frequency bands:\n",
      "1. **Low-Band (Sub-1 GHz)**:\n",
      "   - Offers **better coverage** over long distances but with lower speeds (similar to 4G).\n",
      "   - Used for rural areas and indoor penetration.\n",
      "\n",
      "2. **Mid-Band (1–6 GHz)**:\n",
      "   - Provides a **balance of speed and coverage**, making it the most widely deployed for urban and suburban areas.\n",
      "   - Speeds are significantly faster than 4G but with decent range.\n",
      "\n",
      "3. **High-Band (mmWave, 24 GHz and above)**:\n",
      "   - Delivers **ultra-fast speeds** (up to 20 Gbps) but with **limited range** and poor penetration through walls or obstacles.\n",
      "   - Primarily used in densely populated areas like stadiums or city centers.\n",
      "\n",
      "---\n",
      "\n",
      "### **Applications of 5G**\n",
      "1. **Enhanced Mobile Broadband (eMBB)**:\n",
      "   - Faster internet for smartphones, streaming, and downloads.\n",
      "2. **Massive IoT (mIoT)**:\n",
      "   - Supports a vast number of connected devices, enabling smart homes, cities, and industries.\n",
      "3. **Ultra-Reliable Low-Latency Communications (URLLC)**:\n",
      "   - Critical for applications like autonomous vehicles, remote surgery, and industrial automation.\n",
      "\n",
      "---\n",
      "\n",
      "### **Challenges of 5G**\n",
      "1. **Infrastructure Costs**:\n",
      "   - Deploying 5G requires a dense network of small cells, which is expensive and time-consuming.\n",
      "2. **Spectrum Availability**:\n",
      "   - High-band frequencies (mmWave) require large amounts of spectrum, which is limited and costly.\n",
      "3. **Security Concerns**:\n",
      "   - The increased number of connected devices and higher data rates pose new cybersecurity risks.\n",
      "4. **Health and Environmental Concerns**:\n",
      "   - Some debates exist about the potential health effects of 5G radiation, though regulatory bodies assert it is safe.\n",
      "\n",
      "---\n",
      "\n",
      "### **Global Adoption**\n",
      "- **Leading Countries**: South Korea, China, the U.S., and several European nations are at the forefront of 5G deployment.\n",
      "- **Use Cases**: 5G is already being used in smart factories, augmented reality (AR), virtual reality (VR), and autonomous drones.\n",
      "\n",
      "---\n",
      "\n",
      "### **Future of 5G**\n",
      "5G is expected to be a foundational technology for the **Fourth Industrial Revolution**, enabling innovations like:\n",
      "- Fully autonomous vehicles\n",
      "- Smart cities with real-time data analytics\n",
      "- Immersive AR/VR experiences\n",
      "- Advanced healthcare solutions like remote surgery\n",
      "\n",
      "---\n",
      "\n",
      "Let me know if you'd like to dive deeper into any specific aspect of 5G!"
     ]
    }
   ],
   "source": [
    "# Turn # 4\n",
    "chat_history = run_chatbot(\"What do you know about 5G networks?\", chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0818653-4061-4152-82ea-145eb38f3a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat history:\n",
      "role='USER' message='Hello, I have a question' tool_calls=None \n",
      "\n",
      "role='CHATBOT' message=\"Hello! I'm here to help. Please go ahead and ask your question, and I'll do my best to provide a helpful and informative answer.\" tool_calls=None \n",
      "\n",
      "role='USER' message=\"What's the difference between zero-shot and few-shot prompting\" tool_calls=None \n",
      "\n",
      "role='CHATBOT' message='Great question! Zero-shot and few-shot prompting are techniques used in natural language processing (NLP) to guide language models in generating or classifying text without explicit fine-tuning. Here’s the difference between the two:\\n\\n### **Zero-Shot Prompting**\\n- **Definition**: Zero-shot prompting involves providing the model with a task or question without any examples. The model relies solely on its pre-trained knowledge to generate a response.\\n- **Example**: Asking a model, *\"What is the capital of France?\"* without providing any context or examples of similar questions.\\n- **Use Case**: Useful when the model is expected to generalize its knowledge to new tasks or questions it hasn\\'t explicitly seen before.\\n- **Advantage**: Requires no additional training data or examples, making it efficient for quick queries.\\n- **Limitation**: May not perform well on highly specific or complex tasks where examples could improve understanding.\\n\\n### **Few-Shot Prompting**\\n- **Definition**: Few-shot prompting involves providing the model with a few examples (typically 1–10) of the task or question format before asking it to perform the task. These examples act as a guide for the model.\\n- **Example**: Providing the model with:  \\n  *\"Q: What is the capital of France? A: Paris\"*  \\n  *\"Q: What is the capital of Japan? A: Tokyo\"*  \\n  Then asking: *\"Q: What is the capital of Germany?\"*\\n- **Use Case**: Useful when the task is specific or when the model needs additional context to perform accurately.\\n- **Advantage**: Improves performance on tasks where examples help the model understand the expected format or reasoning.\\n- **Limitation**: Requires crafting relevant examples, which can be time-consuming or require domain knowledge.\\n\\n### **Key Differences**\\n| **Aspect**          | **Zero-Shot**                          | **Few-Shot**                          |\\n|----------------------|----------------------------------------|---------------------------------------|\\n| **Examples Provided**| None                                  | A few (1–10)                         |\\n| **Reliance on Data** | Relies entirely on pre-trained knowledge | Uses examples to guide the model     |\\n| **Complexity**       | Simpler to implement                  | Requires careful selection of examples|\\n| **Performance**      | May struggle with complex tasks       | Generally better for specific tasks  |\\n\\nBoth techniques leverage the model\\'s ability to generalize, but few-shot prompting provides additional context to improve accuracy and relevance. Let me know if you\\'d like further clarification!' tool_calls=None \n",
      "\n",
      "role='USER' message='How would the latter help?' tool_calls=None \n",
      "\n",
      "role='CHATBOT' message='Few-shot prompting helps by providing the language model with a small set of examples that illustrate the task or question format. These examples act as a **contextual guide**, allowing the model to better understand what is expected of it. Here’s how few-shot prompting improves performance:\\n\\n### **1. Clarifies Task Expectations**\\n   - The examples in few-shot prompting explicitly show the model the input-output pairs or the reasoning process required for the task.\\n   - For instance, if you’re asking the model to classify sentiment, providing examples like *\"Text: \\'I love this movie!\\' Sentiment: Positive\"* helps the model understand the format and criteria for classification.\\n\\n### **2. Improves Generalization**\\n   - While the model is pre-trained on a vast corpus of text, few-shot examples help it **adapt** to the specific nuances of the task at hand.\\n   - This is particularly useful for tasks that are not well-represented in the pre-training data or require domain-specific knowledge.\\n\\n### **3. Reduces Ambiguity**\\n   - Without examples, the model might generate responses based on its general understanding, which could be ambiguous or incorrect for specific tasks.\\n   - Few-shot examples **reduce ambiguity** by explicitly demonstrating the desired output style, tone, or structure.\\n\\n### **4. Enhances Consistency**\\n   - By providing examples, you ensure the model’s responses are more consistent with the intended task.\\n   - For example, in a summarization task, examples can show the model the desired length, level of detail, and tone of the summary.\\n\\n### **5. Enables Complex Reasoning**\\n   - For tasks requiring multi-step reasoning or specific logic, few-shot examples can illustrate the thought process.\\n   - For instance, in a math word problem, showing the model how to break down the problem and solve it step-by-step improves its ability to handle similar questions.\\n\\n### **6. Adapts to New Tasks**\\n   - Few-shot prompting is particularly useful for **novel tasks** that the model hasn’t encountered during pre-training.\\n   - By providing a few examples, you can quickly \"teach\" the model how to perform the new task without needing to fine-tune it on a large dataset.\\n\\n### **Example Scenario**\\n**Task**: Translate English to French in a formal tone.  \\n**Few-Shot Examples**:  \\n- *\"English: \\'Thank you for your assistance.\\' French: \\'Merci pour votre assistance.\\'\"*  \\n- *\"English: \\'Could you please provide more details?\\' French: \\'Pourriez-vous fournir plus de détails, s’il vous plaît ?\\'\"*  \\n\\n**Prompt**: *\"English: \\'We appreciate your prompt response.\\' French:\"*  \\n\\n**Outcome**: The model is more likely to produce a formal and accurate translation like *\"Nous apprécions votre réponse rapide.\"* because the examples guided it on tone and structure.\\n\\nIn summary, few-shot prompting acts as a **bridge** between the model’s general knowledge and the specific requirements of the task, leading to more accurate, relevant, and consistent responses.' tool_calls=None \n",
      "\n",
      "role='USER' message='What do you know about 5G networks?' tool_calls=None \n",
      "\n",
      "role='CHATBOT' message=\"Great question! 5G networks represent the fifth generation of wireless communication technology, designed to significantly enhance speed, capacity, and responsiveness compared to previous generations (like 4G LTE). Here’s a breakdown of what you need to know about 5G:\\n\\n---\\n\\n### **Key Features of 5G Networks**\\n1. **Higher Speeds**:\\n   - 5G offers **peak speeds** of up to 20 Gbps (gigabits per second), though real-world speeds are typically lower but still much faster than 4G.\\n   - This enables seamless streaming of 4K/8K video, faster downloads, and improved performance for data-intensive applications.\\n\\n2. **Lower Latency**:\\n   - 5G reduces **latency** (delay in data transmission) to as low as **1 millisecond** (compared to 20–30 ms for 4G).\\n   - This is crucial for real-time applications like autonomous vehicles, remote surgery, and online gaming.\\n\\n3. **Increased Capacity**:\\n   - 5G supports a **higher density of connected devices** per square kilometer, making it ideal for the Internet of Things (IoT) and smart cities.\\n   - It can handle up to **1 million devices per square kilometer**, compared to 4G's limit of around 100,000.\\n\\n4. **Network Slicing**:\\n   - 5G allows **network slicing**, where a single physical network is divided into multiple virtual networks tailored to specific needs (e.g., one slice for IoT devices, another for high-speed data).\\n\\n5. **Improved Reliability**:\\n   - 5G is designed to be more reliable, with **99.999% uptime**, making it suitable for mission-critical applications like industrial automation and emergency services.\\n\\n---\\n\\n### **How 5G Works**\\n5G operates on three main frequency bands:\\n1. **Low-Band (Sub-1 GHz)**:\\n   - Offers **better coverage** over long distances but with lower speeds (similar to 4G).\\n   - Used for rural areas and indoor penetration.\\n\\n2. **Mid-Band (1–6 GHz)**:\\n   - Provides a **balance of speed and coverage**, making it the most widely deployed for urban and suburban areas.\\n   - Speeds are significantly faster than 4G but with decent range.\\n\\n3. **High-Band (mmWave, 24 GHz and above)**:\\n   - Delivers **ultra-fast speeds** (up to 20 Gbps) but with **limited range** and poor penetration through walls or obstacles.\\n   - Primarily used in densely populated areas like stadiums or city centers.\\n\\n---\\n\\n### **Applications of 5G**\\n1. **Enhanced Mobile Broadband (eMBB)**:\\n   - Faster internet for smartphones, streaming, and downloads.\\n2. **Massive IoT (mIoT)**:\\n   - Supports a vast number of connected devices, enabling smart homes, cities, and industries.\\n3. **Ultra-Reliable Low-Latency Communications (URLLC)**:\\n   - Critical for applications like autonomous vehicles, remote surgery, and industrial automation.\\n\\n---\\n\\n### **Challenges of 5G**\\n1. **Infrastructure Costs**:\\n   - Deploying 5G requires a dense network of small cells, which is expensive and time-consuming.\\n2. **Spectrum Availability**:\\n   - High-band frequencies (mmWave) require large amounts of spectrum, which is limited and costly.\\n3. **Security Concerns**:\\n   - The increased number of connected devices and higher data rates pose new cybersecurity risks.\\n4. **Health and Environmental Concerns**:\\n   - Some debates exist about the potential health effects of 5G radiation, though regulatory bodies assert it is safe.\\n\\n---\\n\\n### **Global Adoption**\\n- **Leading Countries**: South Korea, China, the U.S., and several European nations are at the forefront of 5G deployment.\\n- **Use Cases**: 5G is already being used in smart factories, augmented reality (AR), virtual reality (VR), and autonomous drones.\\n\\n---\\n\\n### **Future of 5G**\\n5G is expected to be a foundational technology for the **Fourth Industrial Revolution**, enabling innovations like:\\n- Fully autonomous vehicles\\n- Smart cities with real-time data analytics\\n- Immersive AR/VR experiences\\n- Advanced healthcare solutions like remote surgery\\n\\n---\\n\\nLet me know if you'd like to dive deeper into any specific aspect of 5G!\" tool_calls=None \n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Chat history:\")\n",
    "for c in chat_history:\n",
    "    print(c, \"\\n\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b9a016-e69f-4930-83aa-ef916cb8a278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
